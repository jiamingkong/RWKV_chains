{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RWKV Chains\n",
    "\n",
    "LLMs are generally trained with different datasets and configurations, making them react differently to the same input. In the case of RWKV, because it is not transformer-based, most prompts built inside LangChain are not compatible with it. Therefore we collected and built a set of prompts in `RWKV_chains` to make LangChain work with RWKV."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Load a model first\n",
    "\n",
    "**Please change the `model_name_or_path` to your own model path.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using C:\\Users\\kinet\\AppData\\Local\\torch_extensions\\torch_extensions\\Cache\\py310_cu117 as PyTorch extensions root...\n",
      "Detected CUDA files, patching ldflags\n",
      "Emitting ninja build file C:\\Users\\kinet\\AppData\\Local\\torch_extensions\\torch_extensions\\Cache\\py310_cu117\\wkv_cuda\\build.ninja...\n",
      "Building extension module wkv_cuda...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
      "Loading extension module wkv_cuda...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA compiled\n",
      "RWKV_JIT_ON 1 RWKV_CUDA_ON 1 RESCALE_LAYER 6\n",
      "\n",
      "Loading D:/weights/rwkv/RWKV-4-Raven-7B-v11-Eng99%-Other1%-20230427-ctx8192.pth ...\n",
      "Strategy: (total 32+1=33 layers)\n",
      "* cuda [float16, uint8], store 20 layers\n",
      "* cuda [float16, float16], store 13 layers\n",
      "0-cuda-float16-uint8 1-cuda-float16-uint8 2-cuda-float16-uint8 3-cuda-float16-uint8 4-cuda-float16-uint8 5-cuda-float16-uint8 6-cuda-float16-uint8 7-cuda-float16-uint8 8-cuda-float16-uint8 9-cuda-float16-uint8 10-cuda-float16-uint8 11-cuda-float16-uint8 12-cuda-float16-uint8 13-cuda-float16-uint8 14-cuda-float16-uint8 15-cuda-float16-uint8 16-cuda-float16-uint8 17-cuda-float16-uint8 18-cuda-float16-uint8 19-cuda-float16-uint8 20-cuda-float16-float16 21-cuda-float16-float16 22-cuda-float16-float16 23-cuda-float16-float16 24-cuda-float16-float16 25-cuda-float16-float16 26-cuda-float16-float16 27-cuda-float16-float16 28-cuda-float16-float16 29-cuda-float16-float16 30-cuda-float16-float16 31-cuda-float16-float16 32-cuda-float16-float16 \n",
      "emb.weight                        f16      cpu  50277  4096 \n",
      "blocks.0.ln1.weight               f16   cuda:0   4096       \n",
      "blocks.0.ln1.bias                 f16   cuda:0   4096       \n",
      "blocks.0.ln2.weight               f16   cuda:0   4096       \n",
      "blocks.0.ln2.bias                 f16   cuda:0   4096       \n",
      "blocks.0.att.time_decay           f32   cuda:0   4096       \n",
      "blocks.0.att.time_first           f32   cuda:0   4096       \n",
      "blocks.0.att.time_mix_k           f16   cuda:0   4096       \n",
      "blocks.0.att.time_mix_v           f16   cuda:0   4096       \n",
      "blocks.0.att.time_mix_r           f16   cuda:0   4096       \n",
      "blocks.0.att.key.weight            i8   cuda:0   4096  4096 \n",
      "blocks.0.att.value.weight          i8   cuda:0   4096  4096 \n",
      "blocks.0.att.receptance.weight     i8   cuda:0   4096  4096 \n",
      "blocks.0.att.output.weight         i8   cuda:0   4096  4096 \n",
      "blocks.0.ffn.time_mix_k           f16   cuda:0   4096       \n",
      "blocks.0.ffn.time_mix_r           f16   cuda:0   4096       \n",
      "blocks.0.ffn.key.weight            i8   cuda:0   4096 16384 \n",
      "blocks.0.ffn.receptance.weight     i8   cuda:0   4096  4096 \n",
      "blocks.0.ffn.value.weight          i8   cuda:0  16384  4096 \n",
      "............................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................\n",
      "blocks.31.ln1.weight              f16   cuda:0   4096       \n",
      "blocks.31.ln1.bias                f16   cuda:0   4096       \n",
      "blocks.31.ln2.weight              f16   cuda:0   4096       \n",
      "blocks.31.ln2.bias                f16   cuda:0   4096       \n",
      "blocks.31.att.time_decay          f32   cuda:0   4096       \n",
      "blocks.31.att.time_first          f32   cuda:0   4096       \n",
      "blocks.31.att.time_mix_k          f16   cuda:0   4096       \n",
      "blocks.31.att.time_mix_v          f16   cuda:0   4096       \n",
      "blocks.31.att.time_mix_r          f16   cuda:0   4096       \n",
      "blocks.31.att.key.weight          f16   cuda:0   4096  4096 \n",
      "blocks.31.att.value.weight        f16   cuda:0   4096  4096 \n",
      "blocks.31.att.receptance.weight   f16   cuda:0   4096  4096 \n",
      "blocks.31.att.output.weight       f16   cuda:0   4096  4096 \n",
      "blocks.31.ffn.time_mix_k          f16   cuda:0   4096       \n",
      "blocks.31.ffn.time_mix_r          f16   cuda:0   4096       \n",
      "blocks.31.ffn.key.weight          f16   cuda:0   4096 16384 \n",
      "blocks.31.ffn.receptance.weight   f16   cuda:0   4096  4096 \n",
      "blocks.31.ffn.value.weight        f16   cuda:0  16384  4096 \n",
      "ln_out.weight                     f16   cuda:0   4096       \n",
      "ln_out.bias                       f16   cuda:0   4096       \n",
      "head.weight                       f16   cuda:0   4096 50277 \n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"RWKV_JIT_ON\"] = '1'\n",
    "os.environ[\"RWKV_CUDA_ON\"] = '1' # if '1' then use CUDA kernel for seq mode (much faster)\n",
    "\n",
    "from langchain.llms import RWKV\n",
    "\n",
    "weight_path = \"D:/weights/rwkv/RWKV-4-Raven-7B-v11-Eng99%-Other1%-20230427-ctx8192.pth\"\n",
    "tokenizer_json = \"D:/weights/rwkv/20B_tokenizer.json\"\n",
    "\n",
    "model = RWKV(model=weight_path, strategy=\"cuda fp16i8 *20 -> cuda fp16\", tokens_path=tokenizer_json)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A do-all prompt\n",
    "\n",
    "The Raven versions of RWKV are instruction fined-tuned with Alpaca datasets. Therefore it's much easier if we just include the Alpaca prompt before assigning RWKV to do anything. The Alpaca prompt template is shown below, and we have a `PromptTemplate` called `rwkv_prompt` for ease of use.\n",
    "\n",
    "```text\n",
    "Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
    "# Instruction:\n",
    "{Your instruction or question}\n",
    "\n",
    "# Response:\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rwkv_chains import rwkv_prompt\n",
    "from langchain.chains import LLMChain\n",
    "chain = LLMChain(llm=model, prompt=rwkv_prompt, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mBelow is an instruction that describes a task. Write a response that appropriately completes the request.\n",
      "# Instruction:\n",
      "Why is fusion energy important?\n",
      "\n",
      "# Response:\n",
      "\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\kinet\\anaconda3\\lib\\site-packages\\rwkv\\model.py:625: UserWarning: operator () profile_node %194 : int = prim::profile_ivalue(%192)\n",
      " does not have profile information (Triggered internally at C:\\cb\\pytorch_1000000000000\\work\\third_party\\nvfuser\\csrc\\graph_fuser.cpp:108.)\n",
      "  x, state[i*5+0], state[i*5+1], state[i*5+2], state[i*5+3] = ATT(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Fusion energy is important because it has the potential to provide a clean and sustainable source of energy that can replace fossil fuels. It is considered to be one of the most promising sources of energy in the future, as it has the potential to generate electricity without producing greenhouse gases or pollutants.'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.run(\"Why is fusion energy important?\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summarization with RWKV\n",
    "\n",
    "Inside RWKV, the computation graph resembles RNN in the sense that it has a hidden state that is updated with each token. So in the case of summarization with long texts, the results can be much improved if we put the instruction to summarize **after** the text. The optimized prompt templates can be used via `load_rwkv_summarize_chain`. An example usage is shown below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rwkv_chains import load_rwkv_summarize_chain\n",
    "from langchain.docstore.document import Document\n",
    "\n",
    "summary_chain = load_rwkv_summarize_chain(llm=model, chain_type=\"stuff\", output_key=\"summary\")\n",
    "# map_reduce and refine are also available\n",
    "\n",
    "with open(\"james_webb_news.txt\", \"r\") as f:\n",
    "    text = f.read().strip()\n",
    "docs = [Document(page_content=text)]\n",
    "result = summary_chain.run(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "The Smithsonian’s National Air and Space Museum has named the James Webb Space Telescope the 2023 Michael Collins Trophy for Lifetime and Current Achievements.\n"
     ]
    }
   ],
   "source": [
    "print(result)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Math with RWKV\n",
    "\n",
    "RWKV is also capable of doing math with external tools, here we showcase how RWKV can call python `numexpr` to do math. The optimized prompt template is at `LLM_MATH_PROMPT`. An example usage is shown below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMMathChain chain...\u001b[0m\n",
      "Calculate 458123412 / 4058912"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\kinet\\anaconda3\\lib\\site-packages\\langchain\\chains\\llm_math\\base.py:50: UserWarning: Directly instantiating an LLMMathChain with an llm is deprecated. Please instantiate with llm_chain argument or using the from_llm class method.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32;1m\u001b[1;3m\n",
      "```text\n",
      "458123412 / 4058912\n",
      "```\n",
      "...numexpr.evaluate(\"458123412 / 4058912\")...\n",
      "\u001b[0m\n",
      "Answer: \u001b[33;1m\u001b[1;3m112.86852535851972\u001b[0m\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Answer: 112.86852535851972'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from rwkv_chains import LLM_MATH_PROMPT\n",
    "from langchain.chains import LLMMathChain\n",
    "\n",
    "math_chain = LLMMathChain(llm=model, prompt=LLM_MATH_PROMPT, verbose=True)\n",
    "\n",
    "math_chain.run(\"Calculate 458123412 / 4058912\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bash with RWKV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMBashChain chain...\u001b[0m\n",
      "Show the command to use on Windows to show current date and time\u001b[32;1m\u001b[1;3m\n",
      "# Response:\n",
      "```bash\n",
      "date -d \"now\" +\"%Y-%m-%d %H:%M:%S\"\n",
      "```\n",
      "This command will output the current date and time in the format of YYYY-MM-DD HH:MM:SS. To use this command on Windows, you can use the following command:\n",
      "```bash\n",
      "cmd /c echo %date% %time% > current_date_time.txt\n",
      "```\u001b[0m\n",
      "Code: \u001b[33;1m\u001b[1;3m['date -d \"now\" +\"%Y-%m-%d %H:%M:%S\"', 'cmd /c echo %date% %time% > current_date_time.txt']\u001b[0m"
     ]
    }
   ],
   "source": [
    "from rwkv_chains import LLMBashChain\n",
    "\n",
    "bash_chain = LLMBashChain.from_llm(llm=model, verbose=True)\n",
    "\n",
    "bash_chain.run(\"Show the command to use on Windows to print out the current directory\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
