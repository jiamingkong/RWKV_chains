{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RWKV Chains\n",
    "\n",
    "LLMs are generally trained with different datasets and configurations, making them react differently to the same input. In the case of RWKV, because it is not transformer-based, most prompts built inside LangChain are not compatible with it. Therefore we collected and built a set of prompts in `RWKV_chains` to make LangChain work with RWKV."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Load a model first\n",
    "\n",
    "**Please change the `model_name_or_path` to your own model path.** And it works better with 7B model. 14B is a bit hard to prompt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# suppress the warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"RWKV_JIT_ON\"] = '1'\n",
    "os.environ[\"RWKV_CUDA_ON\"] = '1' # if '1' then use CUDA kernel for seq mode (much faster)\n",
    "\n",
    "from langchain.llms import RWKV\n",
    "\n",
    "weight_path = \"D:/weights/rwkv/RWKV-4-Raven-7B-v11-Eng99%-Other1%-20230427-ctx8192.pth\"\n",
    "# weight_path = \"/home/ubuntu/Documents/Github/raven-weight-7b.pth\"\n",
    "tokenizer_json = \"./20B_tokenizer.json\"\n",
    "\n",
    "model = RWKV(model=weight_path, strategy=\"cuda fp16i8 *20 -> cuda fp16\", tokens_path=tokenizer_json)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A do-all prompt\n",
    "\n",
    "The Raven versions of RWKV are instruction fined-tuned with Alpaca datasets. Therefore it's much easier if we just include the Alpaca prompt before assigning RWKV to do anything. The Alpaca prompt template is shown below, and we have a `PromptTemplate` called `rwkv_prompt` for ease of use.\n",
    "\n",
    "```text\n",
    "Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
    "# Instruction:\n",
    "{Your instruction or question}\n",
    "\n",
    "# Response:\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rwkv_chains import rwkv_prompt\n",
    "from langchain.chains import LLMChain\n",
    "chain = LLMChain(llm=model, prompt=rwkv_prompt, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain.run(\"\"\"The following context is an introduction. Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
    "\n",
    "\"Assessment of visual acuity depends on the optotypes used for measurement. The ability to recognize different optotypes differs even if their critical details appear under the same visual angle. Since optotypes are evaluated on individuals with good visual acuity and without eye disorders, differences in the lower visual acuity range cannot be excluded. In this study, visual acuity measured with the Snellen E was compared to the Landolt C acuity.\",\n",
    "\"100 patients (age 8 - 90 years, median 60.5 years) with various eye disorders, among them 39 with amblyopia due to strabismus, and 13 healthy volunteers were tested. Charts with the Snellen E and the Landolt C (Precision Vision) which mimic the ETDRS charts were used to assess visual acuity. Three out of 5 optotypes per line had to be correctly identified, while wrong answers were monitored. In the group of patients, the eyes with the lower visual acuity, and the right eyes of the healthy subjects, were evaluated.\",\n",
    "\"Differences between Landolt C acuity (LR) and Snellen E acuity (SE) were small. The mean decimal values for LR and SE were 0.25 and 0.29 in the entire group and 0.14 and 0.16 for the eyes with strabismus amblyopia. The mean difference between LR and SE was 0.55 lines in the entire group and 0.55 lines for the eyes with strabismus amblyopia, with higher values of SE in both groups. The results of the other groups were similar with only small differences between LR and SE.\"\n",
    "_________________\n",
    "question:\n",
    "Landolt C and snellen e acuity: differences in strabismus amblyopia?\n",
    "\n",
    "Start with Yes/No/Maybe, then provide a short explain\"\"\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summarization with RWKV\n",
    "\n",
    "Inside RWKV, the computation graph resembles RNN in the sense that it has a hidden state that is updated with each token. So in the case of summarization with long texts, the results can be much improved if we put the instruction to summarize **after** the text. The optimized prompt templates can be used via `load_rwkv_summarize_chain`. An example usage is shown below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rwkv_chains import load_rwkv_summarize_chain\n",
    "from langchain.docstore.document import Document\n",
    "\n",
    "summary_chain = load_rwkv_summarize_chain(llm=model, chain_type=\"stuff\", output_key=\"summary\")\n",
    "# map_reduce and refine are also available\n",
    "\n",
    "with open(\"james_webb_news.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    text = f.read().strip()\n",
    "docs = [Document(page_content=text)]\n",
    "result = summary_chain.run(docs)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# map reduce is also possible, and it can work over a very long article.\n",
    "\n",
    "mr_chain = load_rwkv_summarize_chain(llm=model, chain_type=\"map_reduce\")\n",
    "\n",
    "# this will help when you have very long articles that can't be fed into a forward of the LLM.\n",
    "docs2 = [Document(page_content=i) for i in text.split(\"\\n\\n\")]\n",
    "result4 = mr_chain.run(docs2)\n",
    "print(result4)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Math with RWKV\n",
    "\n",
    "RWKV is also capable of doing math with external tools, here we showcase how RWKV can call python `numexpr` to do math. The optimized prompt template is at `LLM_MATH_PROMPT`. An example usage is shown below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rwkv_chains import LLM_MATH_PROMPT\n",
    "from langchain.chains import LLMMathChain\n",
    "\n",
    "math_chain = LLMMathChain(llm=model, prompt=LLM_MATH_PROMPT, verbose=True)\n",
    "\n",
    "math_chain.run(\"What is 592138 * 4242?\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bash with RWKV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rwkv_chains import LLMBashChain\n",
    "\n",
    "bash_chain = LLMBashChain.from_llm(llm=model, verbose=True)\n",
    "\n",
    "bash_chain.run(\"list all files under current directory.\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Document based QA with RWKV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rwkv_chains import load_rwkv_qa_chain\n",
    "\n",
    "qa_chain = load_rwkv_qa_chain(llm=model, verbose=True, chain_type=\"stuff\")\n",
    "\n",
    "print(qa_chain.run(input_documents = docs, question=\"Who is Nicola Fox?\"))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FAISS powered Document QA with RWKV\n",
    "\n",
    "In this example we will download a 90mb embedding model from `SentenceTransformers`, called `all-MiniLM-L6-v2` and let it work with FAISS to provide some context for the LLM to generate answers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Created a chunk of size 311, which is longer than the specified 200\n",
      "Created a chunk of size 303, which is longer than the specified 200\n",
      "Created a chunk of size 246, which is longer than the specified 200\n",
      "Created a chunk of size 386, which is longer than the specified 200\n",
      "Created a chunk of size 604, which is longer than the specified 200\n",
      "Created a chunk of size 1066, which is longer than the specified 200\n",
      "Created a chunk of size 248, which is longer than the specified 200\n",
      "Created a chunk of size 411, which is longer than the specified 200\n"
     ]
    }
   ],
   "source": [
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.document_loaders import TextLoader\n",
    "from langchain.embeddings import HuggingFaceEmbeddings, SentenceTransformerEmbeddings \n",
    "from rwkv_chains.retrieval_qa import RetrievalQA\n",
    "\n",
    "\n",
    "embeddings = HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n",
    "loader = TextLoader(\"james_webb_news.txt\")\n",
    "documents = loader.load()\n",
    "text_splitter = CharacterTextSplitter(chunk_size=200, chunk_overlap=0)\n",
    "docs = text_splitter.split_documents(documents)\n",
    "\n",
    "db = FAISS.from_documents(docs, embeddings)\n",
    "\n",
    "qa = RetrievalQA.from_chain_type(llm=model, chain_type=\"stuff\", retriever=db.as_retriever(), verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new RetrievalQA chain...\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "The ceremony was held at the Steven F. Udvar-Hazy Center in Chantilly, Virginia.\n"
     ]
    }
   ],
   "source": [
    "print(qa.run(\"Where was the ceremony held?\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
