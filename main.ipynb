{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RWKV Chains\n",
    "\n",
    "LLMs are generally trained with different datasets and configurations, making them react differently to the same input. In the case of RWKV, because it is not transformer-based, most prompts built inside LangChain are not compatible with it. Therefore we collected and built a set of prompts in `RWKV_chains` to make LangChain work with RWKV."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Load a model first\n",
    "\n",
    "**Please change the `model_name_or_path` to your own model path.** And it works better with 7B model. 14B is a bit hard to prompt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# suppress the warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"RWKV_JIT_ON\"] = '1'\n",
    "os.environ[\"RWKV_CUDA_ON\"] = '1' # if '1' then use CUDA kernel for seq mode (much faster)\n",
    "\n",
    "from langchain.llms import RWKV\n",
    "\n",
    "weight_path = \"D:/weights/rwkv/RWKV-4-Raven-7B-v11-Eng99%-Other1%-20230427-ctx8192.pth\"\n",
    "# weight_path = \"/home/ubuntu/Documents/Github/raven-weight-7b.pth\"\n",
    "tokenizer_json = \"./20B_tokenizer.json\"\n",
    "\n",
    "model = RWKV(model=weight_path, strategy=\"cuda fp16i8 *20 -> cuda fp16\", tokens_path=tokenizer_json)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A do-all prompt\n",
    "\n",
    "The Raven versions of RWKV are instruction fined-tuned with Alpaca datasets. Therefore it's much easier if we just include the Alpaca prompt before assigning RWKV to do anything. The Alpaca prompt template is shown below, and we have a `PromptTemplate` called `rwkv_prompt` for ease of use.\n",
    "\n",
    "```text\n",
    "Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
    "# Instruction:\n",
    "{Your instruction or question}\n",
    "\n",
    "# Response:\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rwkv_chains import rwkv_prompt\n",
    "from langchain.chains import LLMChain\n",
    "chain = LLMChain(llm=model, prompt=rwkv_prompt, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain.run(\"Why is fusion energy important?\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summarization with RWKV\n",
    "\n",
    "Inside RWKV, the computation graph resembles RNN in the sense that it has a hidden state that is updated with each token. So in the case of summarization with long texts, the results can be much improved if we put the instruction to summarize **after** the text. The optimized prompt templates can be used via `load_rwkv_summarize_chain`. An example usage is shown below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rwkv_chains import load_rwkv_summarize_chain\n",
    "from langchain.docstore.document import Document\n",
    "\n",
    "summary_chain = load_rwkv_summarize_chain(llm=model, chain_type=\"stuff\", output_key=\"summary\")\n",
    "# map_reduce and refine are also available\n",
    "\n",
    "with open(\"james_webb_news.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    text = f.read().strip()\n",
    "docs = [Document(page_content=text)]\n",
    "result = summary_chain.run(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# map reduce is also possible, and it can work over a very long article.\n",
    "\n",
    "mr_chain = load_rwkv_summarize_chain(llm=model, chain_type=\"map_reduce\")\n",
    "\n",
    "# this will help when you have very long articles that can't be fed into a forward of the LLM.\n",
    "docs2 = [Document(page_content=i) for i in text.split(\"\\n\\n\")]\n",
    "result2 = mr_chain.run(docs2)\n",
    "print(result2)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Math with RWKV\n",
    "\n",
    "RWKV is also capable of doing math with external tools, here we showcase how RWKV can call python `numexpr` to do math. The optimized prompt template is at `LLM_MATH_PROMPT`. An example usage is shown below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rwkv_chains import LLM_MATH_PROMPT\n",
    "from langchain.chains import LLMMathChain\n",
    "\n",
    "math_chain = LLMMathChain(llm=model, prompt=LLM_MATH_PROMPT, verbose=True)\n",
    "\n",
    "math_chain.run(\"What is 592138 * 4242?\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bash with RWKV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rwkv_chains import LLMBashChain\n",
    "\n",
    "bash_chain = LLMBashChain.from_llm(llm=model, verbose=True)\n",
    "\n",
    "bash_chain.run(\"On a windows computer, simply list all files under current directory.\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Document based QA with RWKV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rwkv_chains import load_rwkv_qa_chain\n",
    "\n",
    "qa_chain = load_rwkv_qa_chain(llm=model, verbose=True, chain_type=\"stuff\")\n",
    "\n",
    "qa_chain.run(input_documents = docs, question=\"Did the article also introduced winners back in 1980?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
