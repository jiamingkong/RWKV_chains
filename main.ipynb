{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RWKV Chains\n",
    "\n",
    "LLMs are generally trained with different datasets and configurations, making them react differently to the same input. In the case of RWKV, because it is not transformer-based, most prompts built inside LangChain are not compatible with it. Therefore we collected and built a set of prompts in `RWKV_chains` to make LangChain work with RWKV."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Load a model first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"RWKV_JIT_ON\"] = '1'\n",
    "os.environ[\"RWKV_CUDA_ON\"] = '1' # if '1' then use CUDA kernel for seq mode (much faster)\n",
    "\n",
    "from huggingface_hub import hf_hub_download\n",
    "title = \"RWKV-4-Raven-14B-v12-Eng98%-Other2%-20230523-ctx8192\"\n",
    "model_path = hf_hub_download(repo_id=\"BlinkDL/rwkv-4-raven\", filename=f\"{title}.pth\")\n",
    "\n",
    "\n",
    "from langchain.llms import RWKV\n",
    "\n",
    "model = RWKV(model=model_path, strategy=\"cuda fp16i8 *20 -> cuda fp16\", tokens_path=\"20B_tokenizer.json\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A do-all prompt\n",
    "\n",
    "The Raven versions of RWKV are instruction fined-tuned with Alpaca datasets. Therefore it's much easier if we just include the Alpaca prompt before assigning RWKV to do anything. The Alpaca prompt template is shown below, and we have a `PromptTemplate` called `rwkv_prompt` for ease of use.\n",
    "\n",
    "```text\n",
    "Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
    "# Instruction:\n",
    "{Your instruction or question}\n",
    "\n",
    "# Response:\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rwkv_chains import rwkv_prompt\n",
    "from langchain.chains import LLMChain\n",
    "chain = LLMChain(llm=model, prompt=rwkv_prompt, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain.run(\"Why is fusion energy important?\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summarization with RWKV\n",
    "\n",
    "Inside RWKV, the computation graph resembles RNN in the sense that it has a hidden state that is updated with each token. So in the case of summarization with long texts, the results can be much improved if we put the instruction to summarize **after** the text. The optimized prompt templates can be used via `load_rwkv_summarize_chain`. An example usage is shown below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rwkv_chains import load_rwkv_summarize_chain\n",
    "from langchain.docstore.document import Document\n",
    "\n",
    "summary_chain = load_rwkv_summarize_chain(llm=model, chain_type=\"stuff\", output_key=\"summary\")\n",
    "# map_reduce and refine are also available\n",
    "\n",
    "with open(\"james_webb_news.txt\", \"r\") as f:\n",
    "    text = f.read().strip()\n",
    "docs = [Document(page_content=text)]\n",
    "result = summary_chain.run(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(result)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Math with RWKV\n",
    "\n",
    "RWKV is also capable of doing math with external tools, here we showcase how RWKV can call python `numexpr` to do math. The optimized prompt template is at `LLM_MATH_PROMPT`. An example usage is shown below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rwkv_chains import LLM_MATH_PROMPT\n",
    "from langchain.chains import LLMMathChain\n",
    "\n",
    "math_chain = LLMMathChain(llm=model, prompt=LLM_MATH_PROMPT, verbose=True)\n",
    "\n",
    "math_chain.run(\"What is 458123412 - 6943898?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
