{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RWKV Chains\n",
    "\n",
    "LLMs are generally trained with different datasets and configurations, making them react differently to the same input. In the case of RWKV, because it is not transformer-based, most prompts built inside LangChain are not compatible with it. Therefore we collected and built a set of prompts in `RWKV_chains` to make LangChain work with RWKV."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Load a model first\n",
    "\n",
    "**Please change the `model_name_or_path` to your own model path.** And it works better with 7B model. 14B is a bit hard to prompt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using /home/ubuntu/.cache/torch_extensions/py310_cu117 as PyTorch extensions root...\n",
      "Detected CUDA files, patching ldflags\n",
      "Emitting ninja build file /home/ubuntu/.cache/torch_extensions/py310_cu117/wkv_cuda/build.ninja...\n",
      "Building extension module wkv_cuda...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
      "Loading extension module wkv_cuda...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ninja: no work to do.\n",
      "RWKV_JIT_ON 1 RWKV_CUDA_ON 1 RESCALE_LAYER 6\n",
      "\n",
      "Loading /home/ubuntu/Documents/Github/raven-weight-7b.pth ...\n",
      "Strategy: (total 32+1=33 layers)\n",
      "* cuda [float16, uint8], store 20 layers\n",
      "* cuda [float16, float16], store 13 layers\n",
      "0-cuda-float16-uint8 1-cuda-float16-uint8 2-cuda-float16-uint8 3-cuda-float16-uint8 4-cuda-float16-uint8 5-cuda-float16-uint8 6-cuda-float16-uint8 7-cuda-float16-uint8 8-cuda-float16-uint8 9-cuda-float16-uint8 10-cuda-float16-uint8 11-cuda-float16-uint8 12-cuda-float16-uint8 13-cuda-float16-uint8 14-cuda-float16-uint8 15-cuda-float16-uint8 16-cuda-float16-uint8 17-cuda-float16-uint8 18-cuda-float16-uint8 19-cuda-float16-uint8 20-cuda-float16-float16 21-cuda-float16-float16 22-cuda-float16-float16 23-cuda-float16-float16 24-cuda-float16-float16 25-cuda-float16-float16 26-cuda-float16-float16 27-cuda-float16-float16 28-cuda-float16-float16 29-cuda-float16-float16 30-cuda-float16-float16 31-cuda-float16-float16 32-cuda-float16-float16 \n",
      "emb.weight                        f16      cpu  50277  4096 \n",
      "blocks.0.ln1.weight               f16   cuda:0   4096       \n",
      "blocks.0.ln1.bias                 f16   cuda:0   4096       \n",
      "blocks.0.ln2.weight               f16   cuda:0   4096       \n",
      "blocks.0.ln2.bias                 f16   cuda:0   4096       \n",
      "blocks.0.att.time_decay           f32   cuda:0   4096       \n",
      "blocks.0.att.time_first           f32   cuda:0   4096       \n",
      "blocks.0.att.time_mix_k           f16   cuda:0   4096       \n",
      "blocks.0.att.time_mix_v           f16   cuda:0   4096       \n",
      "blocks.0.att.time_mix_r           f16   cuda:0   4096       \n",
      "blocks.0.att.key.weight            i8   cuda:0   4096  4096 \n",
      "blocks.0.att.value.weight          i8   cuda:0   4096  4096 \n",
      "blocks.0.att.receptance.weight     i8   cuda:0   4096  4096 \n",
      "blocks.0.att.output.weight         i8   cuda:0   4096  4096 \n",
      "blocks.0.ffn.time_mix_k           f16   cuda:0   4096       \n",
      "blocks.0.ffn.time_mix_r           f16   cuda:0   4096       \n",
      "blocks.0.ffn.key.weight            i8   cuda:0   4096 16384 \n",
      "blocks.0.ffn.receptance.weight     i8   cuda:0   4096  4096 \n",
      "blocks.0.ffn.value.weight          i8   cuda:0  16384  4096 \n",
      "............................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................................\n",
      "blocks.31.ln1.weight              f16   cuda:0   4096       \n",
      "blocks.31.ln1.bias                f16   cuda:0   4096       \n",
      "blocks.31.ln2.weight              f16   cuda:0   4096       \n",
      "blocks.31.ln2.bias                f16   cuda:0   4096       \n",
      "blocks.31.att.time_decay          f32   cuda:0   4096       \n",
      "blocks.31.att.time_first          f32   cuda:0   4096       \n",
      "blocks.31.att.time_mix_k          f16   cuda:0   4096       \n",
      "blocks.31.att.time_mix_v          f16   cuda:0   4096       \n",
      "blocks.31.att.time_mix_r          f16   cuda:0   4096       \n",
      "blocks.31.att.key.weight          f16   cuda:0   4096  4096 \n",
      "blocks.31.att.value.weight        f16   cuda:0   4096  4096 \n",
      "blocks.31.att.receptance.weight   f16   cuda:0   4096  4096 \n",
      "blocks.31.att.output.weight       f16   cuda:0   4096  4096 \n",
      "blocks.31.ffn.time_mix_k          f16   cuda:0   4096       \n",
      "blocks.31.ffn.time_mix_r          f16   cuda:0   4096       \n",
      "blocks.31.ffn.key.weight          f16   cuda:0   4096 16384 \n",
      "blocks.31.ffn.receptance.weight   f16   cuda:0   4096  4096 \n",
      "blocks.31.ffn.value.weight        f16   cuda:0  16384  4096 \n",
      "ln_out.weight                     f16   cuda:0   4096       \n",
      "ln_out.bias                       f16   cuda:0   4096       \n",
      "head.weight                       f16   cuda:0   4096 50277 \n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"RWKV_JIT_ON\"] = '1'\n",
    "os.environ[\"RWKV_CUDA_ON\"] = '1' # if '1' then use CUDA kernel for seq mode (much faster)\n",
    "\n",
    "from langchain.llms import RWKV\n",
    "\n",
    "# weight_path = \"D:/weights/rwkv/RWKV-4-Raven-7B-v11-Eng99%-Other1%-20230427-ctx8192.pth\"\n",
    "weight_path = \"/home/ubuntu/Documents/Github/raven-weight-7b.pth\"\n",
    "tokenizer_json = \"./20B_tokenizer.json\"\n",
    "\n",
    "model = RWKV(model=weight_path, strategy=\"cuda fp16i8 *20 -> cuda fp16\", tokens_path=tokenizer_json)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A do-all prompt\n",
    "\n",
    "The Raven versions of RWKV are instruction fined-tuned with Alpaca datasets. Therefore it's much easier if we just include the Alpaca prompt before assigning RWKV to do anything. The Alpaca prompt template is shown below, and we have a `PromptTemplate` called `rwkv_prompt` for ease of use.\n",
    "\n",
    "```text\n",
    "Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
    "# Instruction:\n",
    "{Your instruction or question}\n",
    "\n",
    "# Response:\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rwkv_chains import rwkv_prompt\n",
    "from langchain.chains import LLMChain\n",
    "chain = LLMChain(llm=model, prompt=rwkv_prompt, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mBelow is an instruction that describes a task. Write a response that appropriately completes the request.\n",
      "# Instruction:\n",
      "Why is fusion energy important?\n",
      "\n",
      "# Response:\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Fusion energy is important because it has the potential to provide a virtually limitless, clean source of energy that does not produce greenhouse gases or other pollutants. This could help to reduce our dependence on fossil fuels and decrease our carbon footprint. Additionally, fusion energy has the potential to create a new industry that could create jobs and stimulate economic growth.'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.run(\"Why is fusion energy important?\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summarization with RWKV\n",
    "\n",
    "Inside RWKV, the computation graph resembles RNN in the sense that it has a hidden state that is updated with each token. So in the case of summarization with long texts, the results can be much improved if we put the instruction to summarize **after** the text. The optimized prompt templates can be used via `load_rwkv_summarize_chain`. An example usage is shown below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rwkv_chains import load_rwkv_summarize_chain\n",
    "from langchain.docstore.document import Document\n",
    "\n",
    "summary_chain = load_rwkv_summarize_chain(llm=model, chain_type=\"stuff\", output_key=\"summary\")\n",
    "# map_reduce and refine are also available\n",
    "\n",
    "with open(\"james_webb_news.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    text = f.read().strip()\n",
    "docs = [Document(page_content=text)]\n",
    "result = summary_chain.run(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "NASA's James Webb Space Telescope team has been awarded the 2023 Michael Collins Trophy for Lifetime and Current Achievements, recognizing the team's achievements in pushing the boundaries of space science. The trophy is a miniature version of the iconic \"Web of Space\" sculpture created by artist John Safer.\n"
     ]
    }
   ],
   "source": [
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "NASA's James Webb Space Telescope team has received the 2023 Michael Collins Trophy for Lifetime and Current Achievements, which recognizes their significant contributions to the field of aerospace science and technology. The trophy is named after NASA's first astronaut, who achieved his goal of exploring the moon in 1969. The trophy was created by NASA to recognize those who have made significant contributions to the agency's space missions.\n"
     ]
    }
   ],
   "source": [
    "# map reduce is also possible, and it can work over a very long article.\n",
    "\n",
    "mr_chain = load_rwkv_summarize_chain(llm=model, chain_type=\"map_reduce\")\n",
    "\n",
    "# this will help when you have very long articles that can't be fed into a forward of the LLM.\n",
    "docs2 = [Document(page_content=i) for i in text.split(\"\\n\\n\")]\n",
    "result2 = mr_chain.run(docs2)\n",
    "print(result2)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Math with RWKV\n",
    "\n",
    "RWKV is also capable of doing math with external tools, here we showcase how RWKV can call python `numexpr` to do math. The optimized prompt template is at `LLM_MATH_PROMPT`. An example usage is shown below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/lib/python3.10/site-packages/langchain/chains/llm_math/base.py:50: UserWarning: Directly instantiating an LLMMathChain with an llm is deprecated. Please instantiate with llm_chain argument or using the from_llm class method.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMMathChain chain...\u001b[0m\n",
      "What is 458123412 / 4058912?\u001b[32;1m\u001b[1;3m\n",
      "```text\n",
      "458123412 / 4058912\n",
      "```\n",
      "...numexpr.evaluate(\"458123412 / 4058912\")...\n",
      "\u001b[0m\n",
      "Answer: \u001b[33;1m\u001b[1;3m112.86852535851972\u001b[0m\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Answer: 112.86852535851972'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from rwkv_chains import LLM_MATH_PROMPT\n",
    "from langchain.chains import LLMMathChain\n",
    "\n",
    "math_chain = LLMMathChain(llm=model, prompt=LLM_MATH_PROMPT, verbose=True)\n",
    "\n",
    "math_chain.run(\"What is 458123412 / 4058912?\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bash with RWKV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMBashChain chain...\u001b[0m\n",
      "List all files under current directory\u001b[32;1m\u001b[1;3m\n",
      "# Response:\n",
      "To list all files under the current directory, you can use the `ls` command. To format it in a readable way, you can add spaces between the items and use bullet points to separate them. Here's an example:\n",
      "```bash\n",
      "ls # List all files in the current directory\n",
      "```\n",
      "This will display a list of all files in the current directory, including their names and sizes if they are readable. If you want to see only the file names, you can use the `-l` option:\n",
      "```bash\n",
      "ls -l # List only file names and their sizes\n",
      "```\u001b[0m\n",
      "Code: \u001b[33;1m\u001b[1;3m['ls # List all files in the current directory', 'ls -l # List only file names and their sizes']\u001b[0m\n",
      "Answer: \u001b[33;1m\u001b[1;3m20B_tokenizer.json\n",
      "current_date_time.txt\n",
      "james_webb_news.txt\n",
      "main.ipynb\n",
      "rwkv_chains\n",
      "\u001b[0m\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'20B_tokenizer.json\\ncurrent_date_time.txt\\njames_webb_news.txt\\nmain.ipynb\\nrwkv_chains\\n'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from rwkv_chains import LLMBashChain\n",
    "\n",
    "bash_chain = LLMBashChain.from_llm(llm=model, verbose=True)\n",
    "\n",
    "bash_chain.run(\"List all files under current directory\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Document based QA with RWKV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nThe James Webb Space Telescope (JWST) is a powerful space telescope that will be launched in October 2021 and will be the largest and most powerful space telescope ever created. It will help us better understand our universe and its history, including the origins of life, the evolution of galaxies, and the formation of planets around other stars. JWST is also expected to help us answer some of the biggest questions about our place in the universe, such as whether we are alone in the cosmos.'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from rwkv_chains import load_rwkv_qa_chain\n",
    "\n",
    "qa_chain = load_rwkv_qa_chain(llm=model, verbose=False, chain_type=\"stuff\")\n",
    "\n",
    "qa_chain.run(input_documents = docs2, question=\"What does JWST mean for space exploration?\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retrieval QA with RWKV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import RetrievalQA\n",
    "from langchain.faiss import FAISS\n",
    "from langchain.vectorstores.base import VectorStoreRetriever\n",
    "retriever = VectorStoreRetriever(vectorstore=\n",
    "retrievalQA = RetrievalQA.from_llm(llm=model, retriever=retriever)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
